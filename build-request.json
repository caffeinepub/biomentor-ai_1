{
  "kind": "build_request",
  "title": "Add Live AI Voice Chat Mode (ChatGPT-style) to AI Tutor",
  "projectName": "BioMentor AI",
  "priority": "normal",
  "requirements": [
    {
      "id": "REQ-11",
      "text": "Add a Live Voice Chat tab or mode to the existing AI Tutor page (frontend/src/pages/AITutor.tsx) that enables real-time spoken conversation between the student and the AI biology tutor, similar to ChatGPT's voice mode. The interface must include: a large microphone button to start/stop listening, a visual audio waveform or pulsing animation while the AI is speaking or the user is speaking, a status indicator showing 'Listening…', 'Thinking…', or 'Speaking…' states, and a transcript panel that displays the spoken conversation turns in real time. Use the browser's Web Speech API (SpeechRecognition) to capture the student's voice input and convert it to text, pass that text to the existing bioAI client-side tutor logic to generate a biology response, and use the browser's Web Speech Synthesis API (speechSynthesis) to read the AI response aloud. The voice chat must support interruption — if the student starts speaking while the AI is talking, synthesis stops and recognition restarts immediately.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-4"
        ],
        "quotes": [
          "ai voice chat similar to the chatgpt voice chat where they have live ai voice conversation with the human like that i want"
        ]
      },
      "acceptanceCriteria": [
        "The AI Tutor page displays both a 'Text Chat' tab and a 'Voice Chat' tab; switching between them is seamless.",
        "Clicking the microphone button activates the browser's SpeechRecognition API and shows a 'Listening…' indicator with a visual animation.",
        "After the student finishes speaking, the recognized text appears in the transcript and the status changes to 'Thinking…'.",
        "The AI generates a biology tutor response using the existing bioAI logic and reads it aloud via speechSynthesis, with status showing 'Speaking…' and an animated waveform.",
        "If the student speaks while the AI is talking, the AI stops speaking immediately, recognition restarts, and the new student input is processed.",
        "Each conversation turn (student speech → AI response) is appended to a scrollable transcript panel.",
        "A 'Stop' button ends the voice session and resets the UI to idle state.",
        "If the browser does not support SpeechRecognition or speechSynthesis, a clear unsupported-browser message is shown instead of the voice UI.",
        "Voice chat respects the existing trial/subscription gate — if the trial has expired, accessing voice chat shows the TrialExpiredModal."
      ]
    }
  ],
  "constraints": [
    "Use only browser-native Web Speech API (SpeechRecognition + speechSynthesis) — no external speech/AI service integrations.",
    "Do not modify immutable paths: frontend/src/hooks/useInternetIdentity.ts, frontend/src/hooks/useActor.ts, frontend/src/main.tsx, frontend/src/components/ui.",
    "All Motoko logic must remain in the single actor file backend/main.mo.",
    "Voice chat must reuse the existing bioAI client-side logic from frontend/src/lib/bioAI.ts for response generation."
  ],
  "nonGoals": [
    "No third-party speech-to-text or text-to-speech services (e.g., OpenAI Whisper, Google Speech API).",
    "No backend changes required for voice chat — transcripts are client-side only unless explicitly requested.",
    "No mobile push-to-talk native APIs.",
    "No changes to the existing text chat functionality in AITutor.tsx."
  ],
  "imageRequirements": {
    "required": [],
    "edits": []
  }
}